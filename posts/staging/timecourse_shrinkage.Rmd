---
title: "Timecourse Shrinkage with functional FDR"
author: "Sean Hackett"
date: "9/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

In my last post, I discussed regression and likelihood-based methods for detecting temporal dynamics in timecourse data. These experiments typically involve perturbing a steady-state system at "time zero" and then measuring how the system changes at subsequent time points.

When working with such data I find that its useful to separate modeling into two phases:

1. First detecting timecourses containing signal using statistical methods (the subject of **post**).
2. Then characterize these signals of significant timecourses.

Many methods, such as linear regression, will combine these two steps by both detecting a significant effect with estimating its effect. But the interpretability of these methods comes with the cost that we are applying a somewhat rigid view that variation in our responses is described by the contrasts setup in our design matrix. This is one reason why I favor identifying signals without requiring *a priori* assumption of the types of temporal signals we expect. The Multivariate Gaussian test I previously discussed is one such method. Another approach would be to collect replicates and then applying ANOVA to categorically-encoded timepoints. Both of these tests would allow us to detect arbitrary relationships between time and a response. The downside here is that interpreting changes is difficult, because we only know that a timecourse contains signal but not what the signal looks like.

To characterize signal in signal-containing timecourse we can use methods which will generate interpretable parameters - thereby allowing us to better characterize similarity and differences among features. We could do this using an array of methods which may not have been suitable for identifying signals in the first place. For example, signal processing, mechanistic models, phenomonological models, stats/ML. In the future, I'll discuss the phenomonological [impulse model](https://github.com/calico/impulse) as a way of summarizing changes based on their magnitude and timing. 

Here, I'll discuss a signal processing step that is helpful before modeling the dynamics of individual responses. While we've already identified signal-containing timecourses, not all observations will necessarily contain signal. In perturbation timecourses early measurements may be used to detect rapid changes, but these changes are likely to be rarer than changes manifests later in a timecourse. Because of this, we may trust a weak signal later in a timecourse than an earlier one. This intution can be formalized by assessing signals at an observation level and then applying soft-thresholding to shrink fold-changes towards zero. This shrinkage will be more aggressive for earlier, more dubious observations. This is possible by using the functional false-discvoery rate (fFDR).

First, I'll load some bread-and-butter packages, and then load the perturbation timecourse simulation from **post**.

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)

# ggplot default theme
theme_set(theme_bw())
```

```{r create_timecourse_dataset}
timepts <- c(0, 5, 10, 20, 30, 40, 60, 90) # time points measured
measurement_sd <- 0.5 # standard deviation of Gaussian noise added to each observation

# simulate timecourses from Chechik & Koller impulse model
devtools::source_gist("https://gist.github.com/shackett/e4a1a9b930623580282cd64d33a802c4")

simulated_timecourses <- simulated_timecourses %>%
  filter(signal == "contains signal")
```

As a reminder, we can plot example timecourses drawn from 2K simulated signal-containing timecourses. These timecourses are simulated from an impulse model and will contain one or two sigmoidal responses.

```{r}
example_tcs <- simulated_timecourses %>%
  distinct(tc_id) %>%
  sample_n(5) %>%
  mutate(label = as.character(1:n()))

simulated_timecourses %>%
  inner_join(example_tcs, by = "tc_id") %>%
  ggplot(aes(x = time, color = label)) +
  geom_path(aes(y = sim_fit)) +
  geom_point(aes(y = abundance)) +
  scale_y_continuous("Abundance") +
  scale_color_brewer("Example Timecourse", palette = "Set2") +
  ggtitle("Simulated timecourses containing signal", "line: true values, points: observed values") +
  theme(legend.position = "bottom")
```

The true fold changes are always zero at time zero, and many signals don't begin emerging until late in the time series. This means that the overall magnitude of signal vs. noise tends to increase over time, so we are more likely to suspect that late changes are real.

# estimate the FDR for significant timecourses

In line with the Multivariate Gaussian test used to identify signal-containing timecourses, we will again assume that we know the standard deviation of individual measurements.

Using this information, we can assess how likely an observation is to have come from the underlying noise estimate using a Wald test. Because we are working with fold changes, the variance of fold-changes will be twice the variance of individual observations (the variance of a difference of normals equals the sum of the variances of each normal). With this noise estimate, observed fold change can be compared to the standard deviation estimate to yield a z-score and corresponding p-values.

```{r timecourse_filtering}

# Using non-centered z-statics generated from log-fold change and gene + timecourse noise estimate the local false discovery rate (lfdr) of individual observations
observation_signals <- simulated_timecourses %>%
  # hold out measurement that are exactly zero since they are zero by construction
  dplyr::filter(fold_change != 0) %>%
  dplyr::mutate(z_score = fold_change / sqrt(2*measurement_sd^2)) %>%
  dplyr::mutate(p_norm = 1 - abs(0.5 - pnorm(z_score))*2)
  
ggplot(observation_signals, aes(x = p_norm, y = ..density..)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ time, scale = "free_y") +
  ggtitle("Within significant timecourses, early timepoints are generally noise; later timepoints, generally are signal")
```

Using this approach, we can see that the p-values distributions are closer to $\text{Unif}(0,1)$ at early timepoints and there is a larger fraction of small p-values at late timepoints.

To correct for multiple tests, we could now apply Storey's false discovery rate to control the expected number of false positives (no signal) observations relative to the total number of discoveries that we accept (false positive and true positives). This approach models a p-value distributions as a mixture of true negatives (which will be $\sim\text{Unif}(0,1)$) a true positives (which will tend to have small p-values). The fraction of true negatives is estimated as $\pi_{0}$. Then a cutoff $\lambda$ can be chosen to constrain the FDR at a level (lets say 10\%) based on the ratio of expected false positives ($N\pi_{0}\lambda$) to observations less than $\lambda$ (TPs + FPs).

Because a smaller estimate of $\pi_{0}$ will tend to incrase the $\lambda$ cutoff that controls the FDR thereby increasing the total number of discoveries, we should care about how $\pi_{0}$ is treated. If we were pooling all observations in a time series together then we would in essence say that they have the same $\pi_{0}$ fraction. From the p-value histograms above, we can see that this isn't true. Early timepoints have a high $\pi_{0}$ while later timepoints $\pi_{0}$ is much lower. To account for the differences in $\pi_{0}$ we could estimate the FDR of timepoints separately but this may run into problems if we have relatively few observations for a timepoint. Instead, we can apply the functional FDR to estimate $\pi_{0}$ as a monotonic function of a power surrogate. For this problem, this will mean that we can estimate $\pi_{0}$ as a function of time.

```{r observation_level_fdr}
library(fFDR)

fq <- fqvalue(observation_signals$p_norm, observation_signals$time)

pi_zero_by_t <- estimate_fpi0(p = observation_signals$p_norm,
                              z0 = observation_signals$time)$table %>%
  dplyr::distinct(z0, fpi0) %>%
  dplyr::group_by(z0) %>%
  dplyr::slice(1) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(z0)

qplot(y = pi_zero_by_t$fpi0, x = pi_zero_by_t$z0) +
  scale_x_discrete("Time") +
  scale_y_continuous(expression(pi[0] ~ "= E[F / (T + F)]")) +
  theme_bw()

timepoint_lists <- observation_signals %>%
  plyr::dlply(.variables = "time", dplyr::tbl_df)
# calculate the lfdr for each observation using a timepoint specific lfdr
shrunken_timecourse_estimates <- lapply(timepoint_lists, function(a_time_set) {
  a_time_set$lfdr <- qvalue::lfdr(a_time_set$p_norm, pi0 = pi_zero_by_t$fpi0[pi_zero_by_t$z0 == a_time_set$time[1]])
  a_time_set
}) %>%
  dplyr::bind_rows() %>%
  dplyr::mutate(shrunken_log2_ratio = log2_ratio * (1-lfdr)) %>%
  dplyr::select(TF:gene, tc_level_hard = log2_ratio, tc_level_soft = shrunken_log2_ratio)
```

```{r}
tc_distances <- shrunken_timecourse_estimates %>%
  dplyr::group_by(TF, strain, date, restriction, mechanism, gene) %>%
  dplyr::arrange(desc(time)) %>%
  dplyr::slice(1) %>%
  dplyr::summarize(final_zero_pt = ifelse(tc_level_soft == 0, TRUE, FALSE)) %>%
  dplyr::left_join(overall_timecourse_signal, by = c("TF", "strain", "date", "restriction", "mechanism", "gene"))
ggplot(tc_distances, aes(x = q_chisq, y = ..density.., fill =  final_zero_pt)) +
  geom_density(alpha = 0.5)
```


```{r}
ggplot(shrunken_timecourse_estimates %>%
         dplyr::sample_n(2000), aes(x = tc_level_hard, y = tc_level_soft)) +
  geom_abline(intercept = 0, slope = 1) +
  geom_point(size = 0.2, color = "RED") +
  facet_wrap(~ time) +
  coord_cartesian(xlim = c(-4,4), ylim = c(-4,4)) +
  theme_minimal()
```

