---
title: "Timecourse Shrinkage with functional FDR"
author: "Sean Hackett"
date: "9/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In my last post, I discussed regression and likelihood-based methods for detecting temporal dynamics in timecourse data. These experiments typically involve perturbing a steady-state system at "time zero" and then measuring how the system changes at subsequent time points.

When working with such data I find that its useful to separate modeling into two phases:

1. First detecting timecourses containing signal using statistical methods (the subject of [time zero normalization with the Multivariate Gaussian distribution](http://www.shackett.org/2022/05/08/time_zero_normalization)).
2. Then characterizing the signals of significant timecourses.

Many methods, such as linear regression, will combine these two steps by both detecting a significant effect with estimating its effect. But the interpretability of these methods comes with the cost that we are applying a somewhat rigid view that variation in our responses is described by the contrasts setup in our design matrix. This is one reason why I favor identifying signals without requiring *a priori* assumption of the types of temporal signals we expect. The Multivariate Gaussian test I previously discussed is one such method. Another approach would be to collect replicates and then applying ANOVA to categorically-encoded timepoints. Both of these tests would allow us to detect arbitrary relationships between time and a response. The downside here is that interpreting changes is difficult, because we only know that a timecourse contains signal but not what the signal looks like.

To characterize the patterns in signal-containing timecourse we can use methods which will generate interpretable parameters - thereby allowing us to better characterize similarity and differences among features. We could do this using an array of methods which may not have been suitable for identifying signals in the first place. For example, signal processing, mechanistic models, phenomonological models, stats/ML. In the future, I'll discuss the phenomonological [impulse model](https://github.com/calico/impulse) as a way of summarizing changes based on their magnitude and timing. 

Here, I'll discuss a signal processing step that is helpful before modeling the dynamics of individual responses. While we've already identified signal-containing timecourses, not all observations will necessarily contain signal. In perturbation timecourses early measurements may be collected in order to detect rapid changes, but these changes are likely to be rarer than changes manifesting later in a timecourse. Because of this, we may trust a weak signal later in a timecourse more than an earlier one. This intuiton can be formalized by assessing signals at an observation level and then applying soft-thresholding to shrink fold-changes towards zero. This shrinkage will be more aggressive for earlier, more dubious observations. This is possible by using the functional false-discvoery rate (fFDR).

First, I'll load some bread-and-butter packages, and then load the perturbation timecourse simulation from [time zero normalization with the Multivariate Gaussian distribution](http://www.shackett.org/2022/05/08/time_zero_normalization).

```{r env_setup, message=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)

# ggplot default theme
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r create_timecourse_dataset, message=FALSE}
timepts <- c(0, 5, 10, 20, 30, 40, 60, 90) # time points measured
measurement_sd <- 0.5 # standard deviation of Gaussian noise added to each observation

# simulate timecourses from Chechik & Koller impulse model
devtools::source_gist("https://gist.github.com/shackett/e4a1a9b930623580282cd64d33a802c4")

simulated_timecourses <- simulated_timecourses %>%
  filter(signal == "contains signal")
```

As a reminder, we can plot example timecourses drawn from 2K simulated signal-containing timecourses. These timecourses are simulated from an impulse model and will contain one or two sigmoidal responses.

```{r tc_examples, fig.height = 6, fig.width = 6}
set.seed(1234)

example_tcs <- simulated_timecourses %>%
  distinct(tc_id) %>%
  sample_n(5) %>%
  mutate(label = as.character(1:n()))

simulated_timecourses %>%
  inner_join(example_tcs, by = "tc_id") %>%
  ggplot(aes(x = time, color = label)) +
  geom_path(aes(y = sim_fit)) +
  geom_point(aes(y = abundance)) +
  scale_y_continuous("Fold-Change") +
  scale_color_brewer("Example Timecourse", palette = "Set2") +
  ggtitle("Simulated timecourses containing signal", "line: true values, points: observed values")
```

The true fold changes are always zero at time zero, and many signals don't begin emerging until late in the time series. This means that the overall magnitude of signal vs. noise tends to increase over time, so we are more likely to suspect that late changes are real. We may in turn want to retain these late signals in our dataset while filtering much of the early noise.

# Identifying observations with signal

Before accounting for observations in a timecourse which lack signal, we first need to create a statistic which reflects observation-level signal.

In line with the Multivariate Gaussian test used to identify signal-containing timecourses, we will again assume that we possess a noise estimate for individual measurements (i.e., an estimate of their standard deviation). Since timecourses' abundances were drawn from an isotropic Gaussian (i.e., $\Sigma = \sigma^{2}I), fold-changes relative to time zero can be treated as a difference of normals (recall that the variance of a difference of normals equals the sum of the variances of each Normal distribution).

$$
x_{0} \sim \mathcal{N}(\mu_{0}, \sigma^{2})\\
x_{t} \sim \mathcal{N}(\mu_{t}, \sigma^{2})\\
x_{t} - x_{0} \sim \mathcal{N}(\mu_{t} - \mu_{0}, 2\sigma^{2})
$$
If there is no signal then $\mu_{t} = \mu_{0}$, allowing us to compare fold-change Z-scores to a standard Normal using the Wald test:

$$
x_t - x_0 \sim \mathcal{N}(0, 2\sigma^{2})\\
Z = \frac{x_{t}-x_{0}}{\sqrt{2\sigma^2}}\\
Z \sim \mathcal{N}(0, 1)
$$

We can now estimate the Wald statistics for observed fold-changes. One limitation of this approach is that it doesn't deal with the bias induced by time-zero normalization as discussed in the MVN post; If anyone knows of a better hypothesis test, please ping me :).

```{r timecourse_filtering}
observation_signals <- simulated_timecourses %>%
  # hold out measurements that are exactly zero since they are zero by construction
  dplyr::filter(fold_change != 0) %>%
  dplyr::mutate(z_score = fold_change / sqrt(2*measurement_sd^2)) %>%
  # two-tailed Wald test
  dplyr::mutate(p_norm = 1 - abs(0.5 - pnorm(z_score))*2)

ggplot(observation_signals, aes(x = p_norm, y = ..density..)) +
  geom_histogram(bins = 50, breaks = seq(0, 1, by=0.02)) +
  scale_x_continuous("P-value (Wald test)") +
  ggtitle("Observation-level p-values within signal-containing timecourses")
```
  
Given that we are working with timecourses that contain some signal, its not particularly surprising that many of the observations deviate substantially from the value at time zero. This can be seen from the histogram of p-values pooled across all features and timepoints.

Before digging into the temporal aspect of our data, lets review the anatomy of a p-value distribution (which was binned to create the histogram above) and how to apply FDR methodology to generate a local FDR which is ideal for shrinkage.

# False discovery rate (FDR) review

Using the approach of [Storey & Tibshirani](https://www.pnas.org/content/100/16/9440), we can think about a p-value histogram as a mixture of two distributions:

- Negatives - features with no signal that follow our null distribution and whose p-values will in turn be distributed as $\sim\text{Unif}(0,1)$
- Positives - features containing some signal which will consequently have elevated test statistics and tend towards having small p-values.

To see this visually, we can generate a mini-simulation containing a mixture of negatives and positives.

```{r pvalue_hist_sim, fig.height = 6, fig.width = 10}
set.seed(1234)
n_sims <- 100000
pi0 <- 0.5
beta <- 1.5

simple_pvalue_mixture <- tibble(truth = c(rep("Positive", n_sims * (1-pi0)), rep("Negative", n_sims * pi0))) %>%
  # positives are centered around beta; negatives around 0
  mutate(truth = factor(truth, levels = c("Positive", "Negative")),
         mu = ifelse(truth == "Positive", beta, 0),
         # observations sampled from a normal distribution centered on 0
         # or beta with an SD of 1 (the default)
         x = rnorm(n(), mean = mu),
         # carryout a 1-tailed wald test about 0 
         p = pnorm(x, lower.tail = FALSE))

observation_grob <- ggplot(simple_pvalue_mixture, aes(x = x, fill = truth)) +
  geom_density(alpha = 0.5) +
  ggtitle("Observations with and without signal")

pvalues_grob <- ggplot(simple_pvalue_mixture, aes(x = p, fill = truth)) +
  geom_histogram(bins = 100, breaks = seq(0, 1, by=0.01)) +
  ggtitle("P-values of observations with and without signal")

gridExtra::grid.arrange(observation_grob, pvalues_grob, ncol = 2)
```

While there is a mixture of positive and negative observations, their values cannot be clearly separated (that would be too easy!) rather noise works against some positives, and some negative observations take on extreme values by chance. This is paralleled by the p-values of positive and negative observations. Positive p-values tend to be small, but may also be large; while negative p-values being uniformly distributed from 0 to 1 are as likely to be small as large. The fact that if we have many negative tests we will inevitably have some small p-values is why we should care about correcting for multiple hypotheses and controlling the FDR.

If we want our findings to be reliable then we would like to know how likely they are to be due to chance. For a single test, the p-value captures this notion by calculating the chance that a null value would be as or more extreme than the observed value given that the null hypothesis is true. Extending this to the situation where we may have thousands of p-values, some of which will be small by chance, requires us to think about finding a set of p-values which collectively are enriched for positives relative to negative. The FDR is one measure for this enrichment which involves selecting a set of observation (the positive tests) which constrains the expected number of false positives ($\mathop{{}\mathbb{E}}$FP) selected relative to true positives to a desired proportion: i.e. $\mathop{{}\mathbb{E}}\left[\frac{\text{FP}}{\text{FP} + \text{TP}}\right] \leq \alpha$.

To control the FDR at a level $\alpha$, the Storey procedure first calculates $\hat{\pi}_{0}$, the estimated fraction of null hypothesis (i.e., the 0.5 value from our simulation). This is done by looking at large p-values (near 1). Because large p-values will rarely be signal-containing positives there will be fewer large p-values than would be expected from the number of tests. For example, there are `r sum(simple_pvalue_mixture$p > 0.9)` p-values > 0.9 in our example, which is close to `r n_sims*pi0*0.1`, the value we would expect from $N\pi_{0}*0.1$ (`r n_sims` * $\pi_{0}$ * 0.1). (I'll use the true value of $\pi_{0}$ (`r pi0`) as a stand-in for the estimate of $\hat{\pi}_{0}$ so the numbers are a little clearer.)

Just as we expected `r n_sims*pi0*0.1` null p-values on the interval from [0.9,1], we would expect `r n_sims*pi0*0.1` null p-values on the interval [0,0.1]. But, there are actually `r sum(simple_pvalue_mixture$p < 0.1)` with p-values < 0.1 because positives tend have small p-values. If we chose 0.1 as a possible cutoff, then we would expect `r n_sims*pi0*0.1` false positives while the observed number of p-values < 0.1 equals the denominator of the FDR ($\text{FP} + \text{TP}$). The ratio of these two values, `r n_sims*pi0*0.1/sum(simple_pvalue_mixture$p < 0.1)`, would be the expected FDR at a p-value cutoff of 0.1. Now, we usually don't want to choose a cutoff and then live with the FDR we would get, but rather control the FDR at a level $\alpha$ by tuning the cutoff as a parameter $\lambda$.

To do this we can use the q-value package:

```{r estimate_qvalues}
# install q-value from bioconductor if needed
# remotes::install_bioc("qvalue")

library(qvalue)
qvalue_estimates <- qvalue(simple_pvalue_mixture$p)
```

The q-value object contains an estimate of $\pi_{0}$ of `r qvalue_estimates$pi0` which is close to the truth. It also contains a vector of q-values, lFDR, and other goodies.

The q-values are the quantity that we're usually interested in; if we take all of the q-values less than a target cutoff of say 0.05, then that should give us a set of "discoveries" with less than a 5% FDR.

```{r}
simple_qvalues <- simple_pvalue_mixture %>%
  mutate(q = qvalue_estimates$qvalues)

fdr_pvalue_cutoff <- max(simple_qvalues$p[simple_qvalues$q < 0.05])

simple_qvalues <- simple_qvalues %>%
  mutate(hypothesis_type = case_when(p <= fdr_pvalue_cutoff & truth == "Positive" ~ "TP",
                                     p <= fdr_pvalue_cutoff & truth == "Negative" ~ "FP",
                                     p > fdr_pvalue_cutoff & truth == "Positive" ~ "FN",
                                     p > fdr_pvalue_cutoff & truth == "Negative" ~ "TN"))

hypothesis_type_counts <- simple_qvalues %>%
  count(hypothesis_type)

TP <- hypothesis_type_counts$n[hypothesis_type_counts$hypothesis_type == "TP"]
FP <- hypothesis_type_counts$n[hypothesis_type_counts$hypothesis_type == "FP"]
FDR <- FP / (TP + FP)

knitr::kable(hypothesis_type_counts) %>%
  kableExtra::kable_styling(full_width = FALSE)
```

In this case, due to our simulation, we know whether individual discoveries are true or false positives. As a result we can determine that the realized FDR is `r FDR`, close to our target of 0.05.

In most cases we would take our discoveries and work with them further, confident that as a population, only ~5% of them are bogus. But, in some cases we care about how likely an individual observation is to be a false positive. In this case we can look at the local density of p-values near an observation of interest to estimate a local version of the FDR, the local FDR (lFDR).

# lFDR-based shrinkage

Because the lFDR reflects the relative odds that an observation is null it is a useful measure for shrinkage / thresholding aiming to remove noise and better approximate true value. To do this we can weight an observation by 1-lFDR. One interpretation of this is that we are using the lFDR to hedge our bets between the positive and negatiive mixture components, weighting by our null hypothesis that $\mu$ = 0 with confidence lFDR, and by the alternative $\mu \neq 0$ value of x with confidence 1-lFDR: i.e., $x_{\text{shrinkage}} = \text{lFDR}*0 + (1-\text{lFDR})*x$.

```{r lFDR_shrinkage_ex, fig.height = 6, fig.width = 8}
true_values <- tribble(~ truth, ~ mu,
                       "Positive", beta,
                       "Negative", 0)

shrinkage_estimates <- simple_qvalues %>%
  mutate(lfdr = qvalue_estimates$lfdr,
         xs = x*(1-lfdr)) %>%
  select(truth, x, xs) %>%
  gather(processing, value, -truth) %>%
  mutate(processing = case_when(processing == "x" ~ "original value",
                                processing == "xs" ~ "shrinkage estimate"))

ggplot(shrinkage_estimates, aes(x = value, fill = processing)) +
  geom_density(alpha = 0.5) +
  geom_vline(data = true_values, aes(xintercept = mu), color = "chartreuse", size = 1) +
  facet_grid(truth ~ ., scale = "free_y") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("lFDR-based shrinkage improves agreement between observations and the true mean")
```

Using lFDR-based shrinkage values which are just noise were aggressively shrunk toward their true mean of 0 such that their is very little remaining variation. Positives were shrunk using the same methodology retaining extreme values near their measured value. We can verify that there is an overall decrease in uncertainty about the true mean reflecting the removal of noise.

```{r}
shrinkage_estimates %>%
  inner_join(true_values, by = "truth") %>%
  mutate(resid = value - mu) %>%
  group_by(processing) %>%
  summarize(RSS = sum(resid^2)) %>%
  knitr::kable() %>%
  kableExtra::kable_styling(full_width = FALSE)
```

# Stratifying hypotheses with the functional FDR.

From the previous coverage of the FDR, $\hat{\pi}_{0}$ because $\mathop{{}\mathbb{E}}$FP $\propto$ $\hat{\pi}_{0}$ greatly affecting the number of discoveries we can recover for a target FDR. It will similarly impact lFDR-based shrinkage since the lFDR is also $\propto$ $\hat{\pi}_{0}$. Because of its impact on our interpretation of the FDR and lFDR it is crucial that $\hat{\pi}_{0}$ is estimated accurately. But, our original p-value histogram for observation-level significance from the timecourse dataset, might make us think that we could treat it similarly to the normal mixture simulation, using a fixed value of $\hat{\pi}_{0}$ to estimate the FDR/lFDR. But, we've already noted that there tends to be more signal at later timepoints, this can be seen from the p-value histograms as well.

```{r}
ggplot(observation_signals, aes(x = p_norm, y = ..density..)) +
  geom_histogram(bins = 50, breaks = seq(0, 1, by=0.02)) +
  facet_wrap(~ time, scale = "free_y") +
  ggtitle("Within signal-containing timecourses, early timepoints are generally noise;\nlater timepoints, generally are signal")
```

Visualizing p-value histograms separately by time, we can see that the p-values distributions are closer to $\text{Unif}(0,1)$ at early timepoints and there is a larger fraction of small p-values at late timepoints. This reflects that early timepoints have a high $\pi_{0}$ while later timepoints' $\pi_{0}$ is much lower. To account for the differences in $\pi_{0}$ we could estimate the FDR of timepoints separately but this may run into problems if we have relatively few observations for a timepoint. Instead, we can apply the [functional FDR](https://academic.oup.com/biostatistics/article/22/1/68/5499195) to estimate $\pi_{0}$ as a monotonic function of a power surrogate. For this problem, this will mean that we can estimate $\pi_{0}$ as a function of time.

```{r observation_level_fdr, fig.height = 6, fig.width = 6}
# remotes::install_github("StoreyLab/fFDR")
library(fFDR)

pi_zero_by_t <- estimate_fpi0(p = observation_signals$p_norm,
                              z0 = observation_signals$time,
                              method = "glm")$table %>%
  dplyr::distinct(z0, fpi0) %>%
  dplyr::group_by(z0) %>%
  dplyr::slice(1) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(z0)

qplot(y = pi_zero_by_t$fpi0, x = pi_zero_by_t$z0) +
  scale_x_discrete("Time") +
  scale_y_continuous(expression(pi[0] ~ "= E[F / (T + F)]"), expand = c(0,0)) +
  theme_bw() +
  expand_limits(y = c(0,1))
```

Using the fFDR, as expected the estimated $\pi_{0}$ is high are early timepoints and drops precipitously at later timepoints. This would in turn mean that the lFDRs that we'll use for shrinkage tend to be high at early timepoints and consequently the lFDR-based shrinkage will aggressively shrink observations towards zero. These lFDRs which utilize an appropriate $\pi_{0}$ can be calculated using the *fqvalue* function from lFDR.

```{r timecourse_flfdrs}
shrunken_timecourse_estimates <- fqvalue(p = observation_signals$p_norm,
                  z0 = observation_signals$time,
                  pi0.method = "glm")$table %>%
  bind_cols(observation_signals) %>%
  mutate(shrunken_fold_change = fold_change * (1-lfdr))
```

# Timecourse Shrinkage with functional FDR

With the shrinkage estimates in hand, we can re-examine the original timecourses and compare the shrinkage estimates to the measured observations.

```{r tc_shrinkage_examples, fig.height = 6, fig.width = 6, warning = FALSE}
shrunken_timecourse_estimates %>%
  inner_join(example_tcs, by = "tc_id") %>%
  bind_rows(tibble::tibble(time = 0,
                                  label = as.character(1:5),
                                  sim_fit = 0,
                                  shrunken_fold_change = 0)) %>%
  arrange(label, time) %>%
  ggplot(aes(x = time, color = label)) +
  geom_path(aes(y = sim_fit, group = label)) +
  geom_point(aes(y = shrunken_fold_change), shape = 19, position = position_jitter(width = 1)) +
  geom_point(aes(y = fold_change), shape = 1, position = position_jitter(width = 1)) +
  scale_y_continuous("Shrunken Fold-Change") +
  scale_color_brewer("Example Timecourse", palette = "Set2") +
  ggtitle("Simulated timecourses containing signal", "line: true values, hollow points: observed values, filled points: shrinkage estimates") +
  theme(legend.position = "bottom")
```

From these timecourses we can see that later timepoints' shrinkage estimates are very similar to their observed value while at early timepoints only strong signals are preserved. Its also important to note that because shrinkage preserves later timepoints strong signals preferentially, the kinetics implied by some responses may be shifted towards late timepoints as well. This is a property that is even more pronounced using other soft-thresholding methods such as shrinking all observations towards zero proportionally to the standard deviation.

Like, the normal mixture simulation, we can compare how the original values vs. the shrinkage estimates compare to the true values to see which is a better reconstruction of the underlying signal.

```{r timecourse_shrinkage_RSS, warning = FALSE}
shrunken_timecourse_estimates %>%
  select(sim_fit, fold_change, shrunken_fold_change) %>%
  gather(measure, value, -sim_fit) %>%
  mutate(resid = value - sim_fit) %>%
  group_by(measure) %>%
  summarize(RSS = sum(resid^2)) %>%
  knitr::kable() %>%
  kableExtra::kable_styling(full_width = FALSE)
```

We can also compare the reconstruction across time to see that most of the gains come in cleaning up the early timepoints which were dominated by noise.

```{r timecourse_shrinkage_RSS_by_time, warning = FALSE}
shrunken_timecourse_estimates %>%
  select(time, sim_fit, fold_change, shrunken_fold_change) %>%
  gather(measure, value, -sim_fit, -time) %>%
  mutate(resid = value - sim_fit) %>%
  group_by(measure, time) %>%
  summarize(RSS = sum(resid^2)) %>%
  spread(measure, RSS) %>%
  mutate(fraction_noise_removed = (fold_change - shrunken_fold_change)/fold_change)
```

The shrinkage estimates remove much of the noise at early timepoints while preserving much of the signal as-is. This allows us to use the shrinkage estimates as a drop-in replacements for the original values when further interpreting these signals. In my next post I'll discuss one method for characterizing these signals by representing the kinetics of a timeseries based on the timing and magnitudes of changes using the [impulse R package](https://github.com/calico/impulse).