---
title: "Romic"
description: "An R package for exploratory data analysis of high-dimensional datasets"
author: sean
layout: post
comments: true
tags: [R, software]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Romic is an R package, which I developed, that is now is now available on [CRAN](https://cran.r-project.org/web/packages/romic/index.html). There is already a nice README for romic on [GitHub](https://github.com/calico/romic) and [pkgdown site](https://calico.github.io/romic/articles/romic.html), so here, I will add some context regarding the problems this package addresses.

The first problem we'll consider is that genomics data analysis involves a lot of shuffling between various forms of wide and tall data and incrementally tacking on attributes as needed. Romic aims to simplify this process, by providing a set of flexible data structures that accommodate a range of measurements and metadata and can be readily inter-converted based on the needs of an analysis. 

The second challenge we'll contend with is decreasing the time it takes to generate a plot so that mechanics of plotting rarely interrupt the thought process of data interpretation. Building upon romic's data structure, the meaning of variables (feature-, sample-, measurement-level) are encoded in a schema, so they can be appropriately surfaced to filter or reorder a dataset, and add ggplot2 aesthetics. Interactivity is facilitated using shiny apps composed from romic-centric shiny modules.

Both of these solutions increase the speed, clarity, and succinctness of analysis. I've developed and will continue to refine this package to save myself (and hopefully others!) time. 

While, romic is discussed in the parlance of genomics, romic's data structures are useful for any moderately sized feature-level data, and its interactive visualizations can be used for any data with dense continuous measurements. Because of its generality, romic serves as a useful underlying data structure that can be combined with application-specific schemas and methods to create powerful, succinct workflows. One such application that I'll discuss in a future post, is the [claman](https://github.com/calico/claman) R package which builds upon romic to create an opinionated workflow for mass spectrometry data analysis.

# Data Structures for Genomics

## Conventional Formatting

Datasets in genomics are generally generated and shared in wide formats (one row per gene, one column per sample), often with extra rows and columns added for feature and sample metadata. At first blush this is a good format, because it supports both folks who want to work with a matrix-level dataset as well as individuals who are interested in specific genes.

That said, to manipulate and visualize such data requires integrating metadata with measurements. For example, when correcting for batch effects we often want to incorporate sample-level information, such as the date samples were collected. Combining numeric measurements with categorical and numeric meta-data is awkward in matrices. One could do this with attributes, but generally we would just maintain separate tables for samples, and features, since each variable in a table can have its own class. A benefit of this approach is that working with matrices can be very fast, while the major downsides are having to maintain multiple similar versions of a dataset, and being careful about maintaining the alignment of measurements, features, and samples.

## Romic's Tabular Representations

An alternative to manipulating matrices is to work fully with tabular data. This mode of operation is very similar to working with SQL, allowing us to maintain a complex, yet organized dataset. Using tabular "tidy" data also allow us to tap into the expansive suite of tools in the tidyverse. Working with features, samples, and measurements tables allow us to separately modify each table, while the three tables can be combined (using primary key - foreign key relationships) if we need to add sample- or feature-level context to measurements. 

Romic provides two data structures, a triple_omic an tidy_omic class for representing these two scenarios. These formats can be used interchangeable in romic's functions by treating them as a T*omics (tomic) meta-class. Most exported functions from romic, take a tomic object which means they can convert to whatever format makes most sense for a function under the hood and then return a triple_omic or tidy_omic object depending on the input type.

Using a schema, tables can be combined and then broken apart again without constant guidance, and validators quickly flag data manipulation errors (such as non-unique primary keys, or measurements of the same sample with different sample attributes).

By taking care of many of the joins and reshaping operations that we may have to do, romic helps to simplify analyses while avoid common data manipulation errors. It directly supports dplyr and some ggplot operations, while data can also be easily pulled out of the romic format (and then added back if desired) based on users' needs.

# Exploratory Data Analysis for Genomics

Following a tradition set by Dave Robinson of teaching statistical analysis of genomics data using yeast microarrays [link](http://varianceexplained.org/r/tidy-genomics), I generally teach statistical genomics with the [Brauer et al. 2008](https://www.molbiolcell.org/doi/full/10.1091/mbc.e07-08-0779) dataset and this study formed the basis of romic's [vignette](https://calico.github.io/romic/articles/romic.html) and examples. To expand this theme, here we can look at another old-school yeast expression dataset. 

In [Gasch et al. 2000](https://www.molbiolcell.org/doi/full/10.1091/mbc.11.12.4241) the authors explored how yeast expression depends on a range of stressors. Gasch 2K revealed that regardless of the nature of a stressor, yeast tend to respond to the threat with a relatively stereotypical gene expression response termed the "environmental stress response" (the ESR).

David Botstein (the senior author of both of both the Brauer and Gasch papers) refers to this as "shields up and shields down." The idea is that when the Starship Enterprise is cruising along, most power goes to the engine. But, when the Enterprise is under attack (whether from Klingons, Romulans or asteroids) power needs to be redirected to the shields to combat the threat. Cells behave similarly. An interesting corollary of this behavior is that when addressing one stress (such as desiccation), cells will simultaneously become more resistant to other stressors (such as heat shock).

While humans have more complicated stress sensing pathways than yeast, the mammalian equivalent of the ESR, termed the integrated stress response (ISR), still serves an important role in sensing and responding to diverse stresses. Modulating this pathway is being actively explored as an anti-aging/disease therapy by [Calico](https://longevity.technology/calicos-memory-enhancing-drug-reverses-age-related-mental-decline/#:~:text=ISRIB%20(integrated%20stress%20response%20inhibitor,cognitive%20impairments%20in%20Down's%20Syndrome.), [Denali](https://www.alzforum.org/therapeutics/dnl343) and [Altos](https://altoslabs.com/).

## Data Loading

```{r read_data, message=FALSE, warning=FALSE}
# environment setup
library(dplyr)
library(romic)

gasch_2000 <- readr::read_tsv(
  file = "http://www.shackett.org/files/gasch2000.txt",
  col_types = readr::cols() # to accept default column types
  )
gasch_matrix <- gasch_2000 %>%
  select(-UID, -NAME, -GWEIGHT) %>%
  as.matrix()
rownames(gasch_matrix) <- gasch_2000$UID

print(dim(gasch_matrix))
```

## Process metadata

To interpret any of the patterns in this dataset, we'll need some metadata describing both the measured genes and  samples.

### Genes

Genes are frequently summarized using Gene Ontology (GO) terms that capture their sub-cellular localization (CC), molecular function (MF) or biological process (BP). These are typically one-to-many relationships where a given gene will belong to multiple GO terms in each of three ontologies. GO slim ontologies are a curated subset of GO terms which map each gene to a single BP, MF and CC term.

```{r feature_metadata}
goslim_mappings <- readr::read_tsv(
    "https://downloads.yeastgenome.org/curation/literature/go_slim_mapping.tab",
    col_names = c("ORF", "common", "SGD", "category", "geneset", "GO", "class")
  ) %>%
  select(-GO) %>%
  group_by(ORF, category) %>%
  slice(1) %>%
  tidyr::spread(category, geneset) %>%
  select(
    ORF, common, SGD, class,
    cellular_compartment = C,
    molecular_function = F,
    biological_process = P
  ) %>%
  ungroup()

feature_metadata <- gasch_2000 %>%
  select(UID) %>%
  left_join(goslim_mappings, by = c("UID" = "ORF"))

knitr::kable(feature_metadata %>% dplyr::slice(1:5))
```

### Samples

Working with a fresh dataset invariably involves some data munging to format data and metadata in a usable format. In the case of the Gasch 2,000 dataset, organizing samples was the most painful part of this process. Gasch 2Ks samples are identified with short irregularly formatted names so it requires a bit of work to organize them. We could address this problem with a manually curated spreadsheet (I generally use tibble::tribble() for small tables and Google Sheets for larger one). Luckily, the samples here are still organized enough that we can programmatically summarize them. Samples are defined in two ways: first, by the type of stressor (e.g., heat, starvation, ...) and second, by the severity of the stressor. Within each stressors, samples are arrangedd in order of increasing stress. With this setup, we can capture each stressor using regulator expressions (since there are inconsistencies in the data, such as "diauxic" and "Diauxic").


```{r munging}
library(stringr)

experiment_labels <- tibble::tibble(sample = colnames(gasch_matrix)) %>%
  mutate(experiment = case_when(
    str_detect(sample, "hs\\-1") ~ "Heat Shock (A) (duration)",
    str_detect(sample, "hs\\-2") ~ "Heat Shock (B) (duration)",
    str_detect(sample, "^37C to 25C") ~ "Cold Shock (duration)",
    str_detect(sample, "^heat shock") ~ "Heat Shock (severity)",
    str_detect(sample, "^29C to 33C") ~ "29C to 33C (duration)",
    str_detect(sample, "^29C \\+1M sorbitol to 33C \\+ 1M sorbitol") ~ "29C + Sorbitol to 33C + Sorbitol (duration)",
    str_detect(sample, "^29C \\+1M sorbitol to 33C \\+ \\*NO sorbitol") ~ "29C + Sorbitol to 33C (duration)",
    str_detect(sample, "^constant 0.32 mM H2O2") ~ "Hydrogen peroxide (duration)",
    str_detect(sample, "^1 ?mM Menadione") ~ "Menadione (duration)",
    str_detect(sample, "^2.5mM DTT") ~ "DTT (A) (duration)",
    str_detect(sample, "^dtt") ~ "DTT (B) (duration)",
    str_detect(sample, "diamide") ~ "Diamide (duration)",
    str_detect(sample, "^1M sorbitol") ~ "Sorbitol (duration)",
    str_detect(sample, "^Hypo-osmotic shock") ~ "Hypo-Osmotic Shock (duration)",
    str_detect(sample, "^aa starv") ~ "Amino Acid Starvation (duration)",
    str_detect(sample, "^Nitrogen Depletion") ~ "Nitrogen Depletion (duration)",
    str_detect(sample, "^[Dd]iauxic [Ss]hift") ~ "Diauxic Shift (duration)",
    str_detect(sample, "ypd-2") ~ "YPD (duration)",
    str_detect(sample, "ypd-1") ~ "YPD stationary phase (duration)",
    str_detect(sample, "overexpression") ~ "TF Overexpression",
    str_detect(sample, "car-1") ~ "Carbon Sources (A)",
    str_detect(sample, "car-2") ~ "Carbon Sources (B)",
    str_detect(sample, "ct-1") ~ "Temperature Gradient",
    str_detect(sample, "ct-2") ~ "Temperature Gradient, Steady State"
    )) %>%
  group_by(experiment) %>%
  mutate(experiment_order = 1:n()) %>%
  ungroup() %>%
  mutate(
    experiment_order = ifelse(is.na(experiment), NA, experiment_order),
    experiment = ifelse(is.na(experiment), "Other", experiment)
  )

experiment_labels %>%
  dplyr::sample_n(5) %>%
  knitr::kable()
```

## Formatting for romic

Romic organizes genomic datasets as sets of measurement-, sample-, and feature-level variables. We've essentially created three tables capturing each of these aspects of our dataset already. Romic can bundle these together using a feature primary key shared between the features and measurements table (here, "UID"), and a sample primary key shared between the samples and measurements table (here, "sample").

```{r romic_formatting}
# tidy gasch measurements
tall_gasch <- gasch_2000 %>%
  select(-NAME, -GWEIGHT) %>%
  tidyr::gather("sample", "expression", -UID) %>%
  dplyr::filter(!is.na(expression))
  
triple_omic <- create_triple_omic(
  measurement_df = tall_gasch,
  feature_df = feature_metadata,
  sample_df = experiment_labels,
  feature_pk = "UID",
  sample_pk = "sample"
)
```

# Plotting At the Tips of Your Fingers

When its inefficient to explore a dataset, analyses will either be cursory or take longer than it should. While creating bespoke plots that explore specific aspects of a dataset are difficult to automate, the early stages of exploratory data analysis (EDA) should be. During EDA we hope to identify the major sources of variation in a dataset. Ideally this variation will reflect planned factors in our experimental design, but it is also frequently the case that unexpected sources of variability should be identified so they can be accounted for during modeling.

To support this early exploration, romic provides several specialized and general purpose interactive Shiny apps built form composable Shiny modules. We'll use two of these apps to demonstrate a general workflow where we'll (1) interactively visualize our dataset in Shiny, (2) share the app with colleagues using shinyapp.io (or Rstudio Connect), and (3) create a static visualization which summarizes our findings.

## Deploying a shiny app to shinyapps.io

```{r pcs, message=FALSE, warning=FALSE}
triple_omic$measurements %>%
  dplyr::count(sample) %>%
  dplyr::arrange(n)

#imputed_triple <- triple_omic %>%
#  # overwrite existing expression so that we don't thave the
#  # raw expression changes which contains lots of missing values
#  impute_missing_values(impute_var_name = "expression")
```

```{r add_pcs}
# estimate principal components
triple_omic_w_pcs <- triple_omic %>%
  add_pca_loadings(npcs = 5)

romic::plot_bivariate(triple_omic_w_pcs$samples, x_var = "PC1", y_var = "PC2", color = "experiment")
```

```{r}
app_pcs(triple_omic)
```


```{r}
# filter to a few experiments for the demo
heatshock_triple_omic <- imputed_triple %>%
  filter_tomic(
    filter_type = "category",
    filter_table = "samples",
    filter_variable = "experiment",
    filter_value = c(
      "Heat Shock (A) (duration)",
      "Heat Shock (B) (duration)",
      "Heat Shock (severity)",
      "Temperature Gradient"
      ))

plot_heatmap(heatshock_triple_omic)

app_heatmap(heatshock_triple_omic)
```
