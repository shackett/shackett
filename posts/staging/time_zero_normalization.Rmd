---
title: "Time zero normalization with the Multivariate Gaussian distribution"
author: "Sean Hackett"
date: "9/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro



Timecourses are one of the most powerful experimental designs in biology 

because both the rapid and gradual responses 

I'll use genes as short-hand for whatever features that we might be working with since I've primarily worked with these methods in the context of gene expression data.

Most timecourse experiments 




In this article, I'll evaluate a few established methods for identifying genes which vary across time and then introduce an alternative approach based on the Multivariate Gaussian distribution and Mahalanobis distance. This will lead into future articles focusing on cleaning-up and characterizing the dynamics of genes which vary across time.

## Our timecourse experiment

To evaluate methods for detecting temporal dynamics its helpful to use a dataset where there a clear-cut examples of timecourses with and without signal. With such a dataset in hand, we can easily detect signals that we are missing (false negatives), noise that we think is real (false positives) and evaluate overall recall (what fraction of signals are we detecting). We rarely have such positive and negative examples in real timecourses, so instead we can simulate timecourses with and without signal.

### Environment Setup

First, I'm going to setup the R environment by loading some bread-and-butter packages and setting the global options for future and ggplot2.

```{r env_setup}
# general use packages
library(dplyr)
library(future)
library(ggplot2)
library(tidyr)

# global options
# setup parallelization
plan("multicore")
# ggplot default theme
theme_set(theme_bw())
```

### Simulate timecourses containing signal

First, we can generate the subset of our timecourses which contain signal. These timecourses should follow a broad range of biologically-feasible patterns. 

To construct such timecourses, we can use the phenomonological timecourse model of Chechik & Koller which represents timecourses as a pair of sigmoidal responses, called an impulse, We'll also use a simpler single sigmoidal version of the C & K model. 

To simulate data from these models, we can use the simulate_timecourses() function from the impulse R package, available on GitHub.

This function will draw a set of parameters for sigmoidal and impulse from appropriate distributions to define a simulated timecourse. We'll then add independent normally distributed noise to each observation. (For most genomic data types, measurements are log-normal so we could think of these abundance units as already having been log-transformed).

```{r simulate_timecourses}
timepts <- c(0, 5, 10, 20, 30, 40, 60, 90) # time points measured
measurement_sd <- 0.5 # standard deviation of Gaussian noise added to each observation
total_measurements <- 1000 # total number of genes
signal_frac <- 0.2 # what fraction of genes contain real signal

set.seed(1234)

# simulate timecourses containing signal 

alt_timecourses <- impulse::simulate_timecourses(n = total_measurements * signal_frac * 2,
                                                 timepts = timepts,
                                                 prior_pars = c(v_sd = 0.8,
                                                                rate_shape = 2,
                                                                rate_scale = 0.25,
                                                                time_shape = 1,
                                                                time_scale = 30),
                                                 measurement_sd = measurement_sd) %>%
  unnest_legacy(measurements) %>%
  select(-true_model) %>%
  mutate(signal = "contains signal") %>%
  # drop timecourses where no true value's magnitude is greater than 1 (these
  # aren't really signal containing
  # and timecourses where the initial value isn't ~zero
  group_by(tc_id) %>%
  filter(any(abs(sim_fit) > 1),
         abs(sim_fit[time == 0]) < 0.1) %>%
  ungroup()

# only retain the target number of signal containing timecourses
alt_timecourses <- alt_timecourses %>%
  semi_join(
    alt_timecourses %>%
      distinct(tc_id) %>%
      sample_n(min(n(), total_measurements * signal_frac)),
    by = "tc_id")

knitr::kable(alt_timecourses %>% slice(1:length(timepts)))
```

### Simulate timecourses which are just noise

Timecourses which are just noise are easy to generate, we can just generate these using independent draws from a normal distribution (with the same standard deviation that we used to add noise to the signals).

With timecourses with and without signals in hand, we can combine the two sets together while tracking their origin.

Additionally since we are interested in time-dependent changes with respect to time zero, we can transform abundances into fold changes which subtract the initial value of a timecourse from every measurement. We'll work with both the native abundance scale and time-zero normalized fold changes going forward.

```{r null_timecourses}
null_timecourses <- crossing(tc_id = seq(max(alt_timecourses$tc_id) + 1, total_measurements),
                time = timepts) %>%
  mutate(signal = "no signal",
         sim_fit = 0,
         abundance = rnorm(n(), 0, measurement_sd))

simulated_timecourses <- bind_rows(alt_timecourses, null_timecourses) %>%
  mutate(signal = factor(signal, levels = c("contains signal", "no signal"))) %>%
  group_by(tc_id) %>%
  mutate(fold_change = abundance - abundance[time == 0]) %>%
  ungroup()
```

### Example timecourses 

```{r timecourse_examples}
example_tcs <- simulated_timecourses %>%
  distinct(signal, tc_id) %>%
  group_by(signal) %>%
  sample_n(5) %>%
  mutate(label = as.character(1:n()))

simulated_timecourses %>%
  inner_join(example_tcs, by = c("signal", "tc_id")) %>%
  ggplot(aes(x = time, color = label)) +
  geom_path(aes(y = sim_fit)) +
  geom_point(aes(y = abundance)) +
  facet_wrap(~ signal, ncol = 1, scale = "free_y") +
  scale_y_continuous("Abundance") +
  scale_color_brewer("Example Timecourse", palette = "Set2") +
  ggtitle("Simulated timecourses with and without signal", "line: true values, points: observed values") +
  theme(legend.position = "bottom")
```

## Models to try

```{r}
nested_timecourses <- simulated_timecourses %>%
  nest(timecourse_data = -c(signal, tc_id)) 

nested_timecourses
```

### Linear, quadratic, cubic regression

```{r}
library(broom)
library(furrr)
library(gam)

fit_regression <- function (one_tc, model_fxn = "lm", model_formula, null_formula = NULL) {

  if (all.vars(model_formula)[1] == "fold_change") {
    one_tc <- one_tc %>%
      filter(time != 0)
  }
  
  alt_fit <- do.call(model_fxn, list(data = one_tc, formula = model_formula))
  
  if (model_fxn == "lm") {
    null_fit <- do.call(model_fxn, list(data = one_tc, formula = null_formula))
    model_anova <- anova(null_fit, alt_fit)
  } else {
    model_anova <- alt_fit
  }
  
  model_anova %>%
    broom::tidy() %>%
    filter(!is.na(statistic))
}

standard_models <- nested_timecourses %>%
  mutate(linear_abundance = future_map(timecourse_data, fit_regression, model_fxn = "lm",
                                       model_formula = as.formula(abundance ~ time),
                                       null_formula = as.formula(abundance ~ 1)),
         linear_foldchange = future_map(timecourse_data, fit_regression, model_fxn = "lm",
                                        model_formula = as.formula(fold_change ~ time + 0),
                                        null_formula = as.formula(fold_change ~ 0)),
         cubic_abundance = future_map(timecourse_data, fit_regression, model_fxn = "lm",
                                      model_formula = as.formula(abundance ~ poly(time, degree = 3, raw = TRUE)),
                                      null_formula = as.formula(abundance ~ 1)),
         cubic_foldchange = future_map(timecourse_data, fit_regression, model_fxn = "lm",
                                       model_formula = as.formula(fold_change ~ poly(time, degree = 3, raw = TRUE) + 0),
                                       null_formula = as.formula(fold_change ~ 0)),
         gam_abundance = future_map(timecourse_data, fit_regression, model_fxn = "gam",
                                    model_formula = as.formula(abundance ~ s(time))),
         gam_foldchange = future_map(timecourse_data, fit_regression, model_fxn = "gam",
                                     model_formula = as.formula(fold_change ~ s(time) + 0)))
```

Other models we could have tried.
- mgcv
- autocorrelation

- if we had replicates than a categorical model would make sense


Since time points are not evenly spaced we could have tried transforming time when fitting the above models. While the timepoints are "exponentially" sampled, taking log(time) would send time zero to -Inf so a better transformation would be using the square root of time as the independent variable.




```{r fdr_control}
fdr_control <- function(pvalues) {
  qvalue::qvalue(pvalues)$qvalues 
}

all_model_fits <- standard_models %>%
  select(-timecourse_data) %>%
  gather(model_type, model_data, -tc_id, -signal) %>%
  unnest(model_data) %>%
  group_by(model_type) %>%
  mutate(qvalue = fdr_control(p.value),
         discovery = ifelse(qvalue < 0.1, "positive", "negative")) %>%
  separate(model_type, into = c("model", "response"))

ggplot(all_model_fits, aes(x = p.value, fill = signal)) +
  facet_grid(model ~ response) +
  geom_histogram(bins = 25) +
  scale_fill_brewer(palette = "Set1")
```

```{r}
all_model_fits %>%
  count(signal, model, response, discovery) %>%
  mutate(correct = case_when(signal == "no signal" & discovery == "negative" ~ "true negative",
                             signal == "no signal" & discovery == "positive" ~ "false positive",
                             signal == "contains signal" & discovery == "negative" ~ "false negative",
                             signal == "contains signal" & discovery == "positive" ~ "true positive")) %>%
  select(model, response, correct, n) %>%
  spread(correct, n) %>%
  arrange(response) %>%
  mutate(fdr = `false positive` / (`false positive` + `true positive`),
         recall = `true positive` / (`false negative` + `true positive`)) %>%
  knitr::kable()
```


```{r}
extreme_false_negatives <- all_model_fits %>%
  filter(signal == "contains signal" & response == "abundance" & model %in% c("cubic", "gam")) %>%
  group_by(model, response) %>%
  arrange(desc(qvalue)) %>%
  slice(1:5) %>%
  mutate(label = as.character(1:n())) %>%
  select(tc_id, model, response, label) %>%
  ungroup() %>%
   mutate(facet_label = glue::glue("{response} {model} model false negatives"))

extreme_false_positives <- all_model_fits %>%
  filter(signal == "no signal" & response == "foldchange" & model %in% c("cubic", "gam")) %>%
  group_by(model, response) %>%
  sample_n(5) %>%
  mutate(label = as.character(1:n())) %>%
  select(tc_id, model, response, label) %>%
  ungroup() %>%
  mutate(facet_label = glue::glue("{response} {model} model false positives"))

select_misclassifications <- bind_rows(extreme_false_negatives, 
                                       extreme_false_positives)

simulated_timecourses %>%
  inner_join(select_misclassifications, by = "tc_id") %>%
  ggplot(aes(x = time, color = label)) +
  geom_path(aes(y = sim_fit)) +
  geom_point(aes(y = abundance)) +
  facet_wrap( ~ facet_label, scale = "free_y") +
  scale_y_continuous("Abundance") +
  scale_color_brewer("Example Timecourse", palette = "Set2") +
  ggtitle("Timecourses missed by GAM", "line: true values, points: observed values") +
  theme(legend.position = "bottom")
```

From this we can say a few things:

- When working with abundances we need to include an intercept term so the average value of a feature can be separated from its change over time. Doing this however can remove some early responses since the intercept becomes the point of reference rather than the value at time zero.

- Changes which are showing up primarily in one or two timepoints might be missed since the polynomial and gam models used above can't contort themselves to fit these dynamics. This might be appropriate if these points were just noise but in many cases these are large changes beyond what we would expect as noise in our generative process.

- Working with fold change enforces the value at time zero as the appropriate reference. This makes conceptual sense for a perturbation timecourse since at time zero (and before) the system is in a reference state, and all subsequent timepoints capture the dynamics of interest. However, working directly with fold-changes creates a problem. We are no longer controlling the FDR! In this simulation if we wanted to find discoveries at a 10% FDR than we would in fact be realizing a 60% FDR. This is a big problem, especially since outside of working in a simulation, we don't know which timecourses contain real signal and which are spurious (if that was the case then why would we do the experiment...), so we would would think there were strong signals in our dataset when it may in fact entirely be noise. 

If we wanted to get around these issues, then we could still probably make a regression model work, but it would require adding more samples and cost to the analysis. The two main paths we could take are:

- *replicates of each timecourse* - if we had multiple biological replicates at each timepoint, than rather than treating time as a numerical variable, we could treat it as a categorical variable. In this case we could fit an ANOVA model which would assess whether the variation between timepoints is greater than the variation within timepoints. This would be a powerful way of detecting differences across time which is agnostic to types of changes occurring.

- *denser sampling* - if we had more measurements near timepoints where rapid dynamics were occuring then it would be easier to distinguish smooth rapid responses from single outlier observations. This would still require us to fit a model which can appropriately capture such dynamics, but with more observations we could either fit a more flexible model (with more degrees of freedom) or use the same simple models but with more power to detect significant changes.

In most cases, I think adopting one of these options is probably the smart way to go, however there are reasons why collecting more samples in a given experiment is not feasible such as if MANY similar experiments are being performed, like in IDEA, or if there are constraints on how frequently samples can be collected.

In such cases, I think we can find a path forward by stepping away from regression and thinking about likelihood-based methods which capture the nature of fold changes.

## Timecourse fold change likelihood

Before we posit an appropriate likelihood for fold change, lets figure out why the regression approaches using fold change were so anticonservative. The big problem here was that many timecourses which were just noise looked like they actually contained signal. So, lets work with just the "no signal" timecourses.

```{r corr_vs_tzero}
timecourse_spread <- simulated_timecourses %>%
  filter(signal == "no signal") %>%
  group_by(tc_id) %>%
  mutate(tzero_value = abundance[time == 0]) %>%
  ungroup() %>%
  filter(fold_change != 0) %>%
  select(tc_id, time, tzero_value, fold_change) %>%
  spread(time, fold_change)

ggplot(timecourse_spread, aes(x = `5`, y = `10`, color = tzero_value)) + geom_point() +
  scale_color_gradient2('Time zero value', low = "GREEN", high = "RED", mid = "BLACK", midpoint = 0, breaks = -2:2, limits = c(-2,2)) +
  coord_cartesian(xlim = c(-2,2), ylim = c(-2,2)) +
  theme_minimal()
```

```{r}
false_positive_foldchanges <- simulated_timecourses %>%
  inner_join(select_misclassifications, by = "tc_id") %>%
  filter(response == "foldchange")

false_positive_foldchanges %>%
  ggplot(aes(x = time, y = fold_change, color = label)) +
  geom_point() +
  geom_hline(data = false_positive_foldchanges %>%
               filter(time != 0) %>%
               group_by(label, facet_label) %>%
               summarize(average_fold_change = mean(fold_change)),
             aes(yintercept = average_fold_change, color = label), size = 2) +
  facet_wrap(~ facet_label) +
  scale_color_brewer("Example Timecourse", palette = "Set2")
```

Based on the value of time zero, which all subsequent time points are normalized with respect to, these later timepoints are biased such that they are all higher or lower than otherwise expected. Because of the time zero normalization, in order to test for timecourse-level signal based on the aggregate signal of all observations, we need to account for the dependence of these observations. Luckily, the form of this dependence is quite straight-forward.

We can see this dependence in the sample covariance matrix

```{r}
cov(timecourse_spread[,3:ncol(timecourse_spread)])
```

Observations variances are approximately $2\text{Var}(x_t)$ (2 * `r measurement_sd`) because $\mathcal{N}(\mu_{A}, \sigma^{2}_{A}) - \mathcal{N}(\mu_{B}, \sigma^{2}_{B}) = \mathcal{N}(\mu_{A} - \mu_{B}, \sigma^{2}_{A} + \sigma^{2}_{B})$. Observation covariances are $\text{Var}(\log_2x_t)$ because of the shared normalization to time zero.

Normalization of Normal (or log-Normal) observations to a common reference produces a Multivariate Normal distribution.

$$
z_{it} = \frac{f_{it}}{\sigma_{i}^{(f)}}
$$

$$
\mathbf{z}_{iT} \sim \mathcal{MN}\left(\mu = \mathbf{0}, \Sigma = 
\begin{bmatrix}
    1 & 0.5 & \dots  & 0.5 \\
    0.5 & 1 & \dots  & 0.5 \\
    \vdots & \vdots & \ddots & \vdots \\
    0.5 & 0.5 & \dots  & 1
\end{bmatrix}\right)
$$

```{r}
timecourse_covariance <- matrix(1, nrow = timepts-1, ncol = timepts-1)
diag(timecourse_covariance) <- 2
# simultate draws from multivariate normal
library(mvtnorm)
r_multivariate_normal <- rmvt(10000, sigma = timecourse_covariance, df = 0)
r_multivariate_pvalues <- sapply(1:nrow(r_multivariate_normal), function(i){
  pmvt(lower=rep(-Inf,ncol(r_multivariate_normal)), upper=r_multivariate_normal[i,], sigma = timecourse_covariance, df = 0)
})
r_multivariate_mahalanobis_dist <- mahalanobis(r_multivariate_normal, center = rep(0, times = ncol(timecourse_covariance)), cov = timecourse_covariance, inverted = FALSE)
# test multivariate normality
hist(pchisq(r_multivariate_mahalanobis_dist, df = ncol(timecourse_covariance)), breaks = 50)
# test timecourse samples for multivariate normality
time_course_mahalanobis_dist <- mahalanobis(timecourse_split[,-c(1:2)], center = rep(0, ncol(timecourse_covariance)), cov = timecourse_covariance)
hist(pchisq(time_course_mahalanobis_dist, df = ncol(timecourse_covariance)), breaks = 50)
```

## This test is rather affected by single observation outliers -- we can try calculating distance when holding out different time points

What to do if time zero is the bad array?

```{r eval = FALSE}
timecourses_bad_array <- timecourses %>%
  # add strong batch effect in one time point
  dplyr::mutate(tzero_diff = ifelse(time == 2, tzero_diff + rnorm(n(), 0, 3), tzero_diff)) %>%
  dplyr::ungroup() %>%
  dplyr::filter(time %in% c(2:timepts)) %>%
  dplyr::select(feature, time, tzero_value, tzero_diff)
time_course_mahalanobis_dist_bad_array <- mahalanobis((timecourses_bad_array %>%
  tidyr::spread(time, tzero_diff))[,-c(1:2)], center = rep(0, ncol(timecourse_covariance)), cov = timecourse_covariance)
hist(pchisq(time_course_mahalanobis_dist_bad_array, df = ncol(timecourse_covariance), lower.tail = FALSE), breaks = 50)
held_out_times <- expand.grid(held_out_t = 2:timepts, nonzero_time = 2:timepts, stringsAsFactors = FALSE) %>%
    dplyr::filter(nonzero_time != held_out_t) %>%
    dplyr::left_join(timecourses_bad_array, by = c("nonzero_time" = "time")) %>%
    dplyr::group_by(held_out_t, feature) %>%
    dplyr::mutate(nonzero_time_order = 1:n()) %>%
    dplyr::select(-nonzero_time) %>%
    tidyr::spread(nonzero_time_order, tzero_diff)
held_out_times <- held_out_times %>%
  dplyr::ungroup() %>%
  dplyr::mutate(mahalanobis_d = mahalanobis(held_out_times[4:9], center = rep(0, timepts-2), cov = timecourse_covariance[1:(timepts-2), 1:(timepts-2)])) %>%
  dplyr::group_by(feature) %>%
  dplyr::arrange(feature, mahalanobis_d) %>%
  dplyr::slice(1) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(mahalanobis_p = pchisq(mahalanobis_d, df = timepts-2, lower.tail = FALSE))
hist(held_out_times$mahalanobis_p, breaks = 50)
```

## Load dataset and timecourse-level noise estimates (any estimate of a timecourse's noise can be plugged in)

```{r}
library(dynamicyeast)
suppressPackageStartupMessages(library(dplyr))
library(qvalue)
library(ggplot2)
source("../code/R/2_component_noise.R")
cleaned_dataset <- dynamicyeast::format_expression_data("~/Google Drive/transcription_dynamics/per_time_clean_interp.tsv", is_log_ratio = TRUE)
noise_model <- estimate_gene_fc_noise(cleaned_dataset,
                                     step_fraction = 0.1,
                                     quantile_range_val = 0.95,
                                     n_timepts = 4,
                                     iter_num = 200)
```

```{r}
standardized_fold_changes <- cleaned_dataset %>%
  dplyr::filter(log2_ratio != 0) %>%
  dplyr::left_join(noise_model$gene_noise, by = "gene") %>%
  dplyr::left_join(noise_model$timecourse_noise, by = c("TF", "strain", "date", "restriction", "mechanism")) %>%
  # calculate a z-score for each observation using its gene-level standard deviation
  dplyr::mutate(z_score_fc = log2_ratio / sqrt(gene_var + tc_var)) %>%
  # split timecourses up based on number of timepoints since all timecourse z-scores with the same n will follow the same null distribution
  dplyr::group_by(TF, strain, date, restriction, mechanism, gene) %>%
  dplyr::mutate(nonzero_time_order = 1:n(),
                n_times = n()) %>%
  dplyr::ungroup() %>%
  dplyr::select(-time, -ratio, -log2_ratio, -gene_var, -tc_var) %>%
  plyr::dlply(.variables = "n_times", dplyr::tbl_df)
multivariate_normal_tc_signif <- parallel::mclapply(standardized_fold_changes, function(a_times_set) {
  
  n_times <- a_times_set$n_times[1]
  
  timecourse_spread <- a_times_set %>%
    dplyr::select(-n_times) %>%
    tidyr::spread(nonzero_time_order, z_score_fc)
  
  # pull out time component and test calculate mahalanobis distance
  
  timecourse_matrix <- timecourse_spread %>%
    dplyr::select(-c(TF, strain, date, restriction, mechanism, gene)) %>%
    as.matrix()
  
  tc_sigma <- matrix(0.5, nrow = n_times, ncol = n_times)
  diag(tc_sigma) <- 1
  
  #full_tc_mahalanobis_d <- mahalanobis(timecourse_matrix, center = rep(0, times = n_times), cov = tc_sigma)
  
  # hold out each datapoint seperately and take the minimum distance (check for examples where distance is driven by 1 point)
  
  # generate all timecourses of n-1 size
  #expand.grid(held_out_t = 1:n_times, nonzero_time_order = 1:n_times, stringsAsFactors = FALSE) %>%
  #  dplyr::filter(nonzero_time_order != held_out_t) %>%
  #  dplyr::left_join(a_times_set, by = "nonzero_time_order") %>%
  #  dplyr::group_by(TF, strain, date, restriction, mechanism, gene) %>%
  #  dplyr::mutate(nonzero_time_order = 1:n()) %>%
  #  tidyr::spread(nonzero_time_order, z_score_fc)
  
  timecourse_spread %>%
    dplyr::select(TF, strain, date, restriction, mechanism, gene) %>%
    dplyr::mutate(mahalanobis_d = mahalanobis(timecourse_matrix, center = rep(0, times = n_times), cov = tc_sigma),
                  p_multivariate_normal = pchisq(mahalanobis_d, df = n_times, lower.tail = FALSE))
}, mc.cores = 4) %>%
  dplyr::bind_rows()
  
hist(multivariate_normal_tc_signif$p_multivariate_normal, breaks = 100)
